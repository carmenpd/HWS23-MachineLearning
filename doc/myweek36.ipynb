{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def rsquare(y, ypredict):\n",
    "    return 1-np.sum((y-ypredict)**2)/np.sum((y-np.mean(y))**2)\n",
    "\n",
    "def MSE(y, ypredict):\n",
    "    return 1/len(y)*(y.T-ypredict.T) @ (y-ypredict)\n",
    "\n",
    "# lambda=0 equals the OLS fitting \n",
    "lambda_ = np.append([0],[0.0001*10**i for i in range(0,5)])\n",
    "    \n",
    "n = 100\n",
    "degree = np.array([5, 10, 15])\n",
    "\n",
    "np.random.seed()\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)\n",
    "X = np.column_stack([x**i for i in range(0,degree[len(degree)-1]+1)])\n",
    "\n",
    "MSE_train = np.zeros((len(lambda_),len(degree)))\n",
    "MSE_test = np.zeros((len(lambda_), len(degree)))\n",
    "MSE_OLS_train = np.zeros(len(degree))\n",
    "MSE_OLS_test = np.zeros(len(degree))\n",
    "\n",
    "# scaling\n",
    "# I tried calculating the col_mean with pandas and two other times by hand and somehow I don't get that the mean is 0 for n = 1\n",
    "#   col_mean = np.column_stack([np.mean(X[:,i]) for i in range(0,degree[len(degree)-1]+1)])\n",
    "#   data = pd.DataFrame(X)\n",
    "#   col_mean = data.mean()\n",
    "col_mean = 1/X.shape[0]*sum(X)\n",
    "for i in range(0, len(col_mean)):\n",
    "    X[:,i] = X[:,i] - col_mean[i]\n",
    "\n",
    "# drop intercept\n",
    "X = X[:,1:degree[len(degree)-1]+1]\n",
    "\n",
    "for j in range(0, len(lambda_)):\n",
    "    for i in range(0,len(degree)):\n",
    "\n",
    "        X_tilde = X[:,0:degree[i]]\n",
    "        X_train, X_test, y_train, y_test = train_test_split( X_tilde, y, test_size=0.33, random_state=42)\n",
    "\n",
    "        x = X_train[:,1].reshape((len(X_train)),1)\n",
    "        \n",
    "        #beta\n",
    "        beta = np.linalg.pinv(X_train.T @ X_train + lambda_[j]*np.ones(degree[i])) @ X_train.T @ y_train\n",
    "        #prediction\n",
    "        ypredict = X_train @ beta\n",
    "\n",
    "        MSE_train[j, i] = MSE(y_train, ypredict)\n",
    "        rsquare_ = rsquare(y_train, ypredict)\n",
    "\n",
    "        ypredict_test = X_test @ beta\n",
    "        MSE_test[j, i] = MSE(y_test, ypredict_test)\n",
    "\n",
    "# The limit of lambda going to zero equals OLS, therefore the closer lambda to zero the closer the MSE should be to the OLS MSE\n",
    "# MSE_train is descending in a higher degree, which makes sense because a higher degrees allows a more precise \"interpolation\" of the points\n",
    "# for degree=100 the MSE_train would be 0 but then the MSE_test would be very very high\n",
    "# the MSE_test is ascending in the degrees, which is a sign of overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
